### FlexRAG【压缩专家】
> **压缩专家**：把长篇大论浓缩成精华摘要，而且压缩比例可以根据需要灵活调整，既不丢失关键信息，又能节省存储和处理成本。就像把一本厚书精炼成一份简明扼要的读书笔记。
>

* 发表时间：2024.09.24
* 论文名称：[Lighter And Better: Towards Flexible Context Adaptation For Retrieval Augmented Generation](https://arxiv.org/abs/2409.15699)
* 论文地址：[https://arxiv.org/abs/2409.15699](https://arxiv.org/abs/2409.15699)
* Github 地址：

#### 一、论文动机

现有的RAG系统在处理长篇幅的检索上下文时，需要大量的计算资源进行编码，导致运行成本高昂。为了解决这一问题，FlexRAG提出了一种上下文压缩的方法，旨在减少上下文的数据量，同时尽量保留重要的信息，以提高效率。

#### 二、论文思路

![](20240924_FlexRAG/img/v2-fbae97feccc87a63e3d71dd8abc545e7_1440w.png)

1. **上下文压缩（Context Compression）**：
   - FlexRAG将检索到的上下文压缩成紧凑的嵌入表示。具体来说，FlexRAG在离线阶段对外部文档进行预编码，生成压缩嵌入，并在检索到特定RAG任务的相关文档时对这些压缩嵌入进行下采样。

2. **选择性压缩（Selective Compression）**：
   - FlexRAG通过估计上下文的重要性来实现选择性压缩。具体方法包括：
     - **Token级别估计**：基于LLM对输入提示中令牌的重要性估计。
     - **句子级别估计**：使用通用嵌入器（如E5和BGE）来估计句子与任务提示的相关性。
   - **压缩比分配**：根据估计的重要性对上下文进行分组，并为每组分配不同的压缩比，以平衡压缩效果和上下文信息的保留。

3. **两阶段训练（Two-Stage Training）**：
   - **第一阶段**：在无标签数据上进行自回归预训练，以建立压缩模块与下游LLM之间的初步对齐。预训练的目标函数是最大化基于压缩上下文的语言建模概率。
   - **第二阶段**：使用指令调优数据集进行任务特定的微调，优化RAG任务的答案质量。具体基于问题和压缩检索上下文预测真实答案。

#### 三、实验设计与结果

文章没有详细列出具体的实验结果，但提到了FlexRAG在减少计算资源和提高效率方面的优势。通过上下文压缩和选择性压缩，FlexRAG能够在处理长文本时显著降低计算负担，同时保持较高的性能。

#### 四、论文总结

FlexRAG通过上下文压缩、选择性压缩和两阶段训练，有效地解决了长文本RAG中的计算资源问题。这种方法不仅减少了上下文的数据量，还通过选择性压缩和任务特定的微调，确保了重要信息的保留和模型的高性能。FlexRAG为长文本RAG提供了一种高效且灵活的解决方案，值得进一步研究和应用。

## 致谢

* 参考：[https://mp.weixin.qq.com/s/heYbLVQHeykD1EbqH8PSZw](https://mp.weixin.qq.com/s/heYbLVQHeykD1EbqH8PSZw)