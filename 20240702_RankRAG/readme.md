### RankRAG【全能选手】
> **全能选手**：通过一点特训就能当好"评委"和"选手"双重角色。像个天赋异禀的运动员，只需要少量指导就能在多个项目上超越专业选手，还能把看家本领都融会贯通。
>

* 发表时间：2024.07.02
* 论文名称：[RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/abs/2407.02485)
* 论文地址：[https://arxiv.org/abs/2407.02485](https://arxiv.org/abs/2406.02485)

#### 一、论文动机

- **RAG的挑战**：传统的RAG系统通常依赖于独立的检索和排名模型来从大规模文档集中提取相关信息，然后由语言模型生成答案。这种方法存在局限性，**如检索器的容量限制和顶层上下文的选择权衡，可能导致信息的遗漏或引入不相关的内容，从而影响生成的准确性**.

#### 二、论文思路

![](20240702_RankRAG/img/v2-9dc1a513d404f0e5e184c975a570456a_1440w.png)

- **核心创新**：RankRAG通过指令微调单一的LLM，使其同时具备上下文排名和答案生成的双重功能。这种设计不仅简化了传统RAG系统中多模型的复杂性，还通过共享模型参数增强了上下文的相关性判断和信息的利用效率.
- **训练阶段**：
  1. **监着学习微调（SFT）**：基于多种高质量指令遵循数据集进行微调，提高模型对指令的遵循能力.
  2. **统一指令微调**：同时增强排名和生成能力，通过特定的任务设计（如问答对的相关性判断）训练模型识别和排列相关上下文.
- **推理阶段**：
  1. **检索**：使用检索器从文档集中检索出顶部N个上下文.
  2. **重排**：RankRAG模型重新排序这些上下文，只保留得分最高的k个上下文.
  3. **生成**：这些上下文被用作生成答案的输入.

#### 三、实验设计与结果

- 实验设计与数据集
  - **任务类型**：开放域问答（OpenQA）、事实验证和对话式问答（ConvQA）.
  - **数据集**：包括NQ、TriviaQA、HotpotQA、FEVER等，以及对话式问答数据集如Doc2Dial和TopiOCQA.
  - **基线模型**：包括不使用RAG的LLM（如InstructGPT和PaLM 2）以及使用检索增强的模型（如Atlas和Raven）.
  - **评估指标**：准确率（Accuracy）、精确匹配（Exact Match）和F1分数.
- 主要实验结果与分析
  - **性能提升**：RankRAG在多个知识密集型基准测试中表现出色，特别是在TriviaQA和HotpotQA数据集上，其性能显著优于Llama3-ChatQA-1.5和其他基线模型.
  - **跨领域适应能力**：在未经领域特定训练的情况下，RankRAG在生物医学领域的表现接近专门为该领域训练的模型，展示了其良好的泛化能力和跨领域适应性.
  - **数据效率**：RankRAG在训练中引入少量的排名数据即可实现显著的性能提升，数据效率高，降低了训练成本.

#### 四、论文创新点

- **统一的指令微调框架**：简化了训练流程，提高了模型的通用性，无需依赖独立的排名模型，减少了模型复杂性和运行时开销.
- **高性能**：在多个知识密集型任务和领域中，RankRAG均展现出优于现有RAG模型的性能，特别是在需要处理大量不相关信息的开放领域问答任务中，通过有效的上下文排名显著提高了答案的准确性和相关性.
- **跨领域适应能力**：在未经领域特定训练的情况下，RankRAG在生物医学领域的表现接近专门为该领域训练的模型，证明了其极佳的泛化能力和跨领域适应性，为未来在更多专业领域的应用提供了可能.

#### 五、论文总结

RankRAG的通过指令微调单一的LLM，使其同时具备上下文排名和答案生成的双重功能。通过在训练数据中加入少量排序数据，经过指令微调的大语言模型效果出奇地好，甚至超过了现有的专家排序模型，包括在大量排序数据上专门微调的相同大语言模型。这种设计不仅简化了传统RAG系统中多模型的复杂性，还通过共享模型参数增强了上下文的相关性判断和信息的利用效率。

## 致谢

* 参考：[https://mp.weixin.qq.com/s/BZDXCTKSKLOwDv1j8_75_Q](https://mp.weixin.qq.com/s/BZDXCTKSKLOwDv1j8_75_Q)
