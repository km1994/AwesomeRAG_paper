### FILCO【筛选师】
> **筛选师**：像个严谨的编辑，善于从大量文本中识别并保留最有价值的信息，确保传递给AI的每段内容都精准且相关。
>

* 发表时间：2024.11.14
* 论文名称：[Learning to Filter Context for Retrieval-Augmented Generation](https://arxiv.org/abs/2311.08377)
* 论文地址：[https://arxiv.org/abs/2311.08377](https://arxiv.org/abs/2311.08377)
* Github 地址：[https://github.com/zorazrw/filco](https://github.com/zorazrw/filco)

#### 一、论文动机

检索增强型生成（RAG）在开放域问答、事实验证等知识密集型任务中表现出色，但检索系统并不完美，常常会返回部分或完全不相关的段落。这导致生成模型需要在不完全相关的上下文中生成输出，容易产生幻觉（hallucinations）或错误记忆。为了解决这一问题，FILCO 提出了一种上下文过滤方法，通过细粒度的句子级过滤来提高生成质量。

#### 二、论文思路

- 上下文过滤的核心思想

FILCO 的目标是在生成之前过滤掉不相关的上下文内容，只保留对生成任务真正有用的部分。为此，作者提出了三种方法来选择“有用”的上下文：

1. 字符串包含（String Inclusion, STRINC）：判断文本片段是否包含输出文本，适用于支持文档中包含确切输出文本的情况。
2. 词汇重叠（Lexical Overlap, LEXICAL）：计算示例与候选文本片段之间的单词重叠度，适用于需要基于知识生成回答的任务。
3. 条件互信息（Conditional Cross-Mutual Information, CXMI）：衡量生成模型在有无上下文支持下生成预期输出的概率差异，适用于所有任务。

- 上下文过滤模型的训练

为了在测试时应用上下文过滤，作者训练了一个上下文过滤模型（Mctx），使用上述三种方法选择的上下文作为训练数据。训练数据通过将检索到的段落与查询拼接作为输入，过滤后的上下文作为输出来构建。

- 过滤后的上下文用于生成

在训练阶段，使用过滤后的上下文（tsilver）来训练生成模型（Mgen）。在推理阶段，使用模型预测的过滤上下文（tpred）作为输入，生成最终输出。

#### 三、实验设计与结果

- 实验设计：实验使用了 FLAN-T5 和 LLAMA2 作为基础模型，分别用于上下文过滤和最终生成任务。所有模型均在相同的实验设置下进行微调。
- 实验结果
  - **性能提升**：FILCO 在所有六个任务上均优于基线方法（全段落增强和段落级过滤）。与全段落增强相比，FILCO 在提取式问答任务上平均提升了 4.3%（NQ）和 1.1%（TQA）的精确匹配（EM）分数；在复杂问答任务上提升了 1.0%（HotpotQA）和 0.6%（ELI5）的 F1 分数；在事实验证和对话生成任务上分别提升了 6.2% 和 3.5% 的准确率。
  - **输入长度减少**：FILCO 通过过滤上下文显著减少了输入长度，降低了计算成本。与全段落增强相比，FILCO 将输入长度减少了 44%-64%。
  - **上下文精度提升**：过滤后的上下文在所有任务中都显示出更高的单词精度，尤其是在复杂任务中，如 HotpotQA 和 Wizard of Wikipedia。
- 不同上下文过滤策略的比较
  - **任务适应性**：不同的任务对上下文过滤策略的偏好不同。例如，提取式问答任务（NQ 和 TQA）更适合使用 STRINC；对话生成任务（Wizard of Wikipedia）更适合使用 LEXICAL；而更复杂的任务（如 FEVER、HotpotQA 和 ELI5）更适合使用 CXMI。
  - **模型适应性**：FLAN-T5 和 LLAMA2 在大多数任务上表现一致，但在某些任务上存在差异，如 ELI5。
- **多段落设置下的实验**。作者进一步在多段落设置下验证了 FILCO 的有效性，发现即使在整合多个段落作为上下文输入时，FILCO 仍然优于基线方法，证明了其在复杂任务中的泛化能力。

#### 四、论文总结

FILCO 通过细粒度的上下文过滤显著提高了检索增强型生成模型的性能，减少了不相关或干扰性内容对生成任务的影响。该方法在多个知识密集型任务上表现出色，尤其是在提取式问答、事实验证和对话生成任务中。通过选择合适的上下文过滤策略，FILCO 为不同任务提供了有效的解决方案，为未来知识增强型生成任务的发展提供了新的思路。

## 致谢

* [https://mp.weixin.qq.com/s/oOzFBHS_B7FST6YKynD1GA](https://mp.weixin.qq.com/s/oOzFBHS_B7FST6YKynD1GA)