### FoRAG【作家】
> **作家**：先列写作大纲构思文章框架，再逐段扩充完善内容。同时还配备了一个"编辑"，通过仔细的事实核查和修改建议，帮助完善每个细节，确保作品的质量。
>

* 发表时间：2024.06.19
* 论文名称：[FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering](https://arxiv.org/abs/2406.13779)
* 论文地址：[https://arxiv.org/abs/2406.13779](https://arxiv.org/abs/2406.13779)

#### 一、论文动机

- **RAG的挑战**：尽管RAG通过结合大型语言模型（LLMs）和检索器，利用搜索引擎增强了长篇问答的质量，但生成的长形答案中存在两个关键问题：**事实性不足和逻辑清晰度不够**.
- **FoRAG的目标**：提出一种新颖的大纲增强生成器（outline-enhanced generator）和基于精心设计的事实性优化方法，以实现多面性答案的清晰逻辑生成，并提高答案的事实性.

#### 二、论文思路

1. **大纲增强生成器（Outline-Enhanced Generator）**：
   - **两阶段生成技术**：首先生成一个组织模式和大纲，以提高生成答案的逻辑清晰度。第一阶段，生成器使用大纲模板，根据用户查询和上下文草拟答案大纲。第二阶段基于生成的大纲扩展每个观点，构建最终答案.
   - **大纲阶段（Outline Stage）**：生成器根据问题和上下文，选择最合适的组织模式（如因果关系、比较对比等），然后输出一个包含关键点的大纲.
   - **扩展阶段（Expansion Stage）**：根据前一阶段生成的大纲，LLM扩展每个关键点，形成完整的答案.

2. **新颖的事实性优化方法**：
   - **双细粒度RLHF框架**：为了克服直接应用RLHF（Reinforcement Learning from Human Feedback）的困难，提出了一个双细粒度的RLHF框架。该框架通过在事实性评估和奖励建模两个核心步骤中引入细粒度设计，提供了更密集的奖励信号.
   - **细粒度评估**：三种评估粒度：
     - **整体性**：将整个答案作为一个单元进行评估.
     - **句子级别**：将答案分割成句子，并分别评估每个句子.
     - **子声明级别**：进一步将每个句子分解为多个子声明，并分别评估每个子声明.
   - **细粒度奖励建模**：两种奖励建模粒度：
     - **序列级别**：为每个序列学习单一的奖励，反映相应序列的事实性.
     - **标记级别**：为序列中的每个标记学习奖励，通过聚合所有标记级别的奖励来计算序列的奖励.
   - **PPO优化**：采用近端策略优化（PPO）方法来优化生成模型，通过最大化训练好的奖励模型提供的奖励.

#### 三、实验设计与结果

- **性能比较**：通过广泛的实验，验证了所提出的事实性优化RAG（FoRAG）方法在英文和中文基准测试中的优越性。特别是，当将该方法应用于Llama2-7B-chat时，得到的模型FoRAG-L-7B在连贯性、有用性和事实性这三个常用指标上超过了WebGPT-175B，而FoRAG-L-7B的参数数量仅为WebGPT-175B的1/24.
- **有无提纲增强和事实性优化的FoRAG变体的比较**：实验结果表明，提纲增强和事实性优化显著提高了生成答案的质量.
- **不同事实性优化技术的性能比较**：实验结果表明，双细粒度的RLHF框架在优化事实性方面优于传统的RLHF方法.

#### 四、论文总结

FoRAG提出了一种新颖的大纲增强生成器，在第一阶段生成器使用大纲模板，根据用户查询和上下文草拟答案大纲，第二阶段基于生成的大纲扩展每个观点，构建最终答案。同时提出一种基于精心设计的双精细粒度RLHF框架的事实性优化方法，通过在事实性评估和奖励建模两个核心步骤中引入细粒度设计，提供了更密集的奖励信号。

## 致谢

* 参考：[https://mp.weixin.qq.com/s/7uqZ5U10Ec2Pa7akCLCJEA](https://mp.weixin.qq.com/s/7uqZ5U10Ec2Pa7akCLCJEA)