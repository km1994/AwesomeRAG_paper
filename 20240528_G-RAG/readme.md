### G-RAG【串门神器】
> **串门神器**：不再是单打独斗地查资料，而是给每个知识点都建立人际关系网。像个社交达人，不仅知道每个朋友的特长，还清楚谁和谁是酒肉朋友，找答案时直接顺藤摸瓜。
>

* 发表时间：2024.05.28
* 论文名称：[Don't Forget to Connect! Improving RAG with Graph-based Reranking](https://arxiv.org/abs/2405.18414)
* 论文地址：[https://arxiv.org/abs/2405.18414](https://arxiv.org/abs/2405.18414)

#### 一、论文动机

- **RAG系统的挑战**：传统的RAG方法在处理信息不全或上下文关联度较低的文档时，存在文档间连接利用不足的问题，导致过拟合和计算资源消耗增加，影响系统性能.
- **G-RAG的目标**：通过结合文档图和AMR图，改善现有重排序技术，减少过拟合并降低计算资源消耗，从而提高RAG系统的性能.

#### 二、论文思路

![](20240528_G-RAG/img/v2-2d5ebe07d3b96eabee00d50ccbe02be8_1440w.png)

- **文档图的构建**：
   - 对每个问题-文档对，将问题和文档文本拼接后使用AMR BART解析生成AMR图.
   - 基于这些AMR图构建无向文档图，其中节点表示文档，边表示文档之间的共同概念.
   - 通过消息传递机制更新边特征.
- **节点特征的生成**：
   - 使用预训练语言模型对所有检索到的文档进行编码，得到初始节点表示.
   - 通过识别从“question”节点到其他节点的最短单源路径（SSSP），提取路径上的节点概念，构建AMR信息序列，并将其与文档文本拼接后再次编码，得到最终的节点特征.
- **边特征的生成**：
   - 计算文档之间的共同节点数和共同边数，作为边的特征，并进行归一化处理.
- **图神经网络的表示更新**：
   - 基于初始节点和边特征，使用图神经网络进行多轮迭代更新，通过聚合邻居节点的特征来更新每个节点的表示.
- **重排序得分和训练损失**：
   - 使用余弦相似度计算每个文档的重排序得分，并使用交叉熵损失函数和成对排名损失函数进行训练。成对排名损失函数能够更好地处理排名分数的平局情况.

#### 三、实验设计与结果

- 实验设计
  - **数据集**：
     - 使用自然问题（NQ）和TriviaQA数据集进行实验。NQ数据集包含来自Google搜索查询的文本块，而TQA数据集包含来自trivia和quiz-league网站的问答对.
  - **模型细节**：
     - 使用DPR进行文档检索，生成100个文档，并使用AMRBART解析每个问题-文档对生成AMR图.
     - 采用2层图卷积网络（GCN）作为GNN模型，隐藏维度从{8, 64, 128}中选择，dropout率从{0.1, 0.2, 0.4}中选择，学习率从{5e-5, 1e-4, 5e-4}中选择.
     - 使用AdamW优化器，学习率5e-5至1e-4，批量大小5，总训练步数50k，每10k步评估一次.
  - **评估指标**：
     - 使用平均倒数排名（MRR）、平均命中率@10（MHits@10）及其变体（MTRR和TMHits@10）评估重排序性能.
- 实验结果
  - **重排序性能**：
     - G-RAG方法在NQ和TQA数据集上的MRR和MHits@10指标上均优于现有方法。
     - 在NQ数据集上，G-RAG的MRR达到了27.3，而BART的MRR为25.7；在TQA数据集上，G-RAG的MHits@10达到了42.9，而BART的MHits@10为38.5.
  - **PaLM 2评估**：
     - 使用PaLM 2作为重排序器时，发现其在重排序任务中的表现不如G-RAG。这可能是由于PaLM 2生成的相关性得分中存在大量平局，导致排名效果不佳.
     - 使用提出的MTRR和TMHits@10指标进行评估，结果显示G-RAG方法显著提高了重排性能.
  - **不同嵌入模型的比较**：
     - 在相同设置下，Ember模型表现最佳，尤其是在引入超参数调优后，其MRR达到了28.9。其次是GTE和BGE，而BART-GST和BERT表现略低.

#### 四、论文创新点

1. 提出了一种基于文档图的重新排序器G-RAG，利用文档之间的连接信息提高RAG的性能，特别是在文档与问题上下文连接较弱的情况下.
2. 引入了新的评估指标MTRR和TMHits@10，更公平地评估各种排名场景，包括得分相同的排名情况.
3. 评估了PaLM 2作为重排序器的性能，发现即使使用最先进的预训练大型语言模型，重排序器模型设计在RAG中的重要性也不容忽视.
4. G-RAG在保持较小计算足迹的同时，优于现有的最先进方法.
5. 通过识别有助于重新排序器识别更相关文档的关键因素，而不是将所有AMR相关的标记都添加为节点级特征，从而提高了性能并避免了过拟合.

#### 五、论文总结

RAG 在处理文档与问题上下文的关系时仍存在挑战，当文档与问题的关联性不明显或仅包含部分信息时，模型可能无法有效利用这些文档。此外，如何合理推断文档之间的关联也是一个重要问题。 G-RAG实现了RAG检索器和阅读器之间基于图神经网络（GNN）的重排器。该方法结合了文档之间的连接信息和语义信息（通过抽象语义表示图），为 RAG 提供了基于上下文的排序器。
