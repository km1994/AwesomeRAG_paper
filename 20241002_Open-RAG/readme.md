### CoTKR【图谱翻译】
> **图谱翻译**：像个耐心的老师，先理解知识的来龙去脉，再一步步讲解，不是简单复述而是深入浅出地转述。同时通过不断收集"学生"的反馈来改进自己的讲解方式，让知识传递更加清晰有效。
>

* 发表时间：2024.09.29
* 论文名称：[CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering](https://arxiv.org/abs/2409.19753)
* 论文地址：[https://arxiv.org/abs/2409.19753](https://arxiv.org/abs/2409.19753)
* Github 地址：[https://github.com/wuyike2000/CoTKR](https://github.com/wuyike2000/CoTKR)

#### 一、论文动机

大语言模型（LLMs）在自然语言处理任务中表现出色，但在知识密集型任务如问答（QA）中仍会遇到事实性错误，即“幻觉”问题。为了解决这一挑战，检索增强生成（RAG）范式通过从外部源检索与任务相关的知识来增强LLMs的能力。在RAG范式下，知识图谱（KGs）作为信息源，可以增强LLMs在QA任务中的能力。然而，现有方法在将检索到的子图转换为LLMs能理解的自然语言时，存在冗余、遗漏关键细节或语义不匹配的问题。

#### 二、论文思路

- 研究方法

CoTKR方法的核心是以交错方式生成推理路径和相应知识，克服单步知识改写的限制。具体来说，CoTKR包括以下两个主要操作：

1. **推理**：分解问题以识别推理所需的知识。
2. **总结**：根据推理步骤的输出，总结检索到的三元组中的相关知识。

CoTKR通过将思维链（CoT）与知识改写整合，过滤掉无关信息并提取与问题相关的知识，生成与问题语义对齐的、组织良好的知识表示。

- 训练框架
CoTKR的训练框架包括两个阶段：

1. **监督微调**：使用ChatGPT生成的参考知识表示来指导知识改写器的监督式微调，使其初步掌握知识改写的能力。
2. **基于问答反馈的偏好对齐（PAQAF）**：通过利用QA模型的反馈进一步优化知识改写器，弥合知识改写器和QA模型之间的偏好差异。具体步骤包括：
   - **候选知识表示形式采样**：从知识改写器中采样多个候选知识表示。
   - **基于问答模型反馈的偏好注释**：选择两个具有最大语义差异的知识表示，评估其问答性能，识别偏好的知识表示。
   - **基于ChatGPT的数据增强**：利用ChatGPT对偏好的知识表示进行释义，增强训练数据的多样性。
   - **直接偏好优化（DPO）**：使用DPO对知识改写器进行微调，优化知识表示的质量。

#### 三、实验设计与结果

实验在GrailQA和GraphQuestions两个数据集上进行，使用Llama-2 (7B)、Llama-3 (8B)和ChatGPT 2进行知识改写，同时使用ChatGPT和Mistral (7B)执行问答任务。实验结果表明，CoTKR在多个评价指标和大语言模型中均优于基准线，证明了其知识改写策略的有效性。

#### 四、论文总结

1. **性能提升**：CoTKR在大多数评价指标和大语言模型中均优于基准线，显著提高了LLMs在KGQA中的性能。
2. **偏好对齐**：CoTKR+PA在性能上接近甚至超过了ChatGPT作为知识改写器，证明了训练框架和偏好对齐的有效性。
3. **知识表示的重要性**：精心设计的知识表示形式对于在KGQA中使用的大语言模型至关重要。CoTKR+PA始终优于Triple，表明精心设计的知识表示形式可以有效提升KGQA的性能。
4. **检索方法的影响**：设计一个高质量的检索器仍然是一个开放性问题。GS显著优于BM25和2-Hop，表明检索噪声对KGQA性能的影响重大。
5. **数据增强的有效性**：数据增强增强了偏好对齐的效果，CoTKR+PA表现最佳，证明了数据增强的有效性。



## 致谢

* 参考：[https://mp.weixin.qq.com/s/lCHxLxRP96Y3mofDVjKY9w](https://mp.weixin.qq.com/s/lCHxLxRP96Y3mofDVjKY9w)